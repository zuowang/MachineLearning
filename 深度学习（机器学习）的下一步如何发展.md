作者：郑华滨
链接：https://www.zhihu.com/question/47602063/answer/106988408
来源：知乎
著作权归作者所有，转载请联系作者获得授权。

提几个可能的发展方向，同时也就是尚待解决的问题：

1.让深度学习自动调超参。最近看到有人在一个AI群里推广自己的一篇论文《Deep Q-Networks for Accelerating the Training of Deep Neural Networks》https://arxiv.org/abs/1606.01467，大致是用强化学习的方法训练一个控制器来自动控制学习率以及在一个batch中各个类的样本占比。虽然它那篇论文问题很大，训练出来的控制器极其不通用，只能用在它原本的任务上，但是感觉很容易解决掉，这个另说。想象一下，如果能够训练出一个通用的控制器，对于各类任务都能够自动调整超参（或者只在某个子领域比如图像分类做到通用也好），那我们就再也不用自称调参狗了，同时也可以解放出更多的时间用于设计模型、验证架构，想必深度学习的发展步伐会得到极大加速。

2.自动学习网络架构。其实说起来这个问题也可以归入自动调超参，但是感觉应该还是有很大的不同。说起来无非就是两个方面，一是加法二是减法。加法方面可以参考《Net2Net: Accelerating Learning via Knowledge Transfer》https://arxiv.org/abs/1511.05641，这篇是让CNN自动根据需要自动拓展架构，包括横向的增加filter和纵向的增加layer。减法方面可以参考各类Network Compression（网络压缩）的论文中的所谓Network Pruning（网络剪枝），比如《Deep Compression - Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding》http://arxiv.org/abs/1510.00149，虽然这些论文出发点不在于自动学习网络架构而在于压缩网络规模，而且它们往往是在训练收敛之后才对网络进行裁剪而非边训练边裁剪，但是感觉只需要再跨一步就可以了。我个人觉得，自动学习网络架构需要解决的最根本问题就是“应该在什么时机进行架构变动”以及“应该怎么变”，第二个问题感觉上述论文算是回答得可以了，但是第一个问题似乎还有很多可以探索的地方。对于第一个问题，似乎强化学习就很适合解决，因为显然可以把它看成一个控制问题。

3.迁移学习。众所周知，深度学习的直接训练依赖大量数据，而transfer和finetune能够有效利用数据量大的外部任务训练出来特征来迁移到数据量小的目标任务上，使得目标任务对于数据量的要求大大减小。现在的问题在于，迁移学习的思想现在大家其实都在用，很多论文中都可以看到finetune的做法，但是对于两个任务之间需要“多像”才能够迁移这么一个问题还没有一个很好的回答。即使我们不奢求能够给出一个严格的数学理论，至少，如果有人能够做一个非常系统的对比实验，总结出一些规律，使得我们有信心说在如何如何这样一个边界内的任务都是基本上可以transfer的，那将会是一个很大的进步。这个问题也可以这么看，如今我们应该有信心说两个图像分类任务可以transfer，但是这个边界太过狭窄，我个人期待的就是能够有一套理论或者方法论使得这个边界大大拓展，然后在这个边界内我们可以像对两个图像分类任务一样自信满满地用迁移学习。

4.无监督／半监督学习。像LeCun等大佬其实一直在鼓吹这方面，但似乎还没有搞出像当年CNN（AlexNet）、最近强化学习（阿法狗）这样级别的大新闻来。我理解在这个问题上的努力方向应该是确定“何种representation最有用”。具体来说，就是找到一个指标，然后用深度网络优化这个指标，使得满足这个指标的data representation能够具有非常好的特性。再具体一些，下面举三个实际例子：
autoencoder以重构损失作为指标来学习一个representation。
之前听一个讲座，演讲人介绍他的论文《Why Deep Learning Works: A Manifold Disentanglement Perspective》IEEE Xplore Abstract，其中定义了三个指标来描述深度网络每一层中data representation的“蜷曲程度”，并发现，越高层的数据蜷曲度越低，换言之，越平展。那么无监督学习是否能够直接以这个蜷曲度作为损失函数来学习一个representation呢？
这篇论文《Context Encoders: Feature Learning by Inpainting》提出通过预测周边上下文像素来无监督学习视觉特征，感觉很像word2vec从一维变成二维。
除了上述的重构损失、蜷曲度、预测上下文精度，还有没有别的指标学习出来的representation更好呢？个人认为这些问题就是推动无监督／半监督学习进展的关键所在。

5.基于外部存储（external memory）的模型。如果说RNN、LSTM这样的模型属于internal memory / long-term memory的话，那么以神经图灵机（Neural Turing Machine，http://arxiv.org/abs/1410.5401）、记忆网络（Memory Network，http://arxiv.org/abs/1410.3916）为代表的模型就应该称为external memory / really long-term memory了。不过这两个模型刚出来的时候还太过naive，只能做一些很无聊的task，比如序列复制和排序以及非常简单的QA，但是现在已经开始看到它们被用到更加实际的问题上面，例如One-shot Learning：《One-shot Learning with Memory-Augmented Neural Networks》，http://arxiv.org/abs/1605.06065。往大了说，如果未来要实现强AI，这种外部存储的机制肯定是必不可少的。现在的问题在于，神经图灵机和记忆网络用的外部存储虽然比LSTM那样简单的一个hidden state向量更进一步，但也其实就是很简单的一片矩阵，没有任何结构和层次可言，换言之，就是还不够复杂。所以我猜想接下来可能external memory会和知识图谱（Knowledge Graph）结合起来或至少是向知识图谱类似的做法靠拢，因为知识图谱更加结构化。
